{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d93d22",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "DEVICE = \"cpu\"\n",
    "EMBED_DIM = 300\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61aede",
   "metadata": {},
   "source": [
    "### Get the tokenizer\n",
    "- Different models tolenize in different ways. \n",
    "    - Word2Vec / GloVe does words (WordLevel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e3b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic english tokenizer using the get_tokenizer function.\n",
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4b78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove this.\n",
    "assert(len(basic_english_tokenizer(\"This is some text ...\")) == 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505cf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed later.\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64096cd8",
   "metadata": {},
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should loop over the (label, text) data pair and tokenize the text.\n",
    "# It should yield a list of the tokens for each text.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f48f23ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95812\n"
     ]
    }
   ],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "# Use build_vocab_from_iterator to build the vocabulary.\n",
    "# This function should take yield_tokens.\n",
    "# The special characters are PAD and UNK.\n",
    "# Build the vocabulary with the special tokens PAD and UNK\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),specials = [PAD, UNK])\n",
    "\n",
    "#Print the vocabulary -> too big so i only printed the length \n",
    "print(len(vocab.get_stoi()))\n",
    "# Make the default index the same as that of the unk_token.\n",
    "default_index = stoi[UNK]\n",
    "vocab.set_default_index(default_index)\n",
    "\n",
    "# word -> int mapping.\n",
    "stoi = vocab.get_stoi()\n",
    "# int -> word mapping.\n",
    "itos = vocab.get_itos() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840252c4",
   "metadata": {},
   "source": [
    "### Get GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9a0c",
   "metadata": {},
   "source": [
    "Information about pretrained vectors: \n",
    "- https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe\n",
    "- https://github.com/pytorch/text/blob/e3799a6eecef451f6e66c9c20b6432c5f078697f/torchtext/vocab/vectors.py#L263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4589bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GLOVE to the name='840B' GloVe vectors of dimension 300. \n",
    "GLOVE = GloVe(name ='840B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10318bc7",
   "metadata": {},
   "source": [
    "If the embeddings are not in the token space, a zero vector will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ddde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for all the tokens in s = \"Hello, How are you?\"\n",
    "# Look up \"get_vecs_by_tokens\" for GloVe vectors.\n",
    "# Add an assertion checking that the dimensions of wat you get is dimension (???, 300).\n",
    "s = \"Hello, How are you?\"\n",
    "s_tokens = TOKENIZER(s)\n",
    "ret = GLOVE.get_vecs_by_tokens(s_tokens, lower_case_backup=True)\n",
    "assert(ret.shape==(len(s_tokens),300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df54d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let s = \"\"<pad> <unk> the man Man ahsdhashdahsdhash\".\n",
    "# What are the vectors of each token. Print this below.\n",
    "s = \"<pad> <unk> the man Man ahsdhashdahsdhash\"\n",
    "s_tokens = TOKENIZER(s)\n",
    "ret = GLOVE.get_vecs_by_tokens(s_tokens, lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b05fc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391c2a8",
   "metadata": {},
   "source": [
    "These functions tokenize the string input and then map each token to the integer representation in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16ca1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return for a sentence the int tokens for that sentence.\n",
    "# I.e., you pass in \"a b c d\" and get out [1, 2, 3, 4].\n",
    "def text_pipeline(text):\n",
    "    return vocab(TOKENIZER(text))\n",
    "\n",
    "# Return the label starting at 0. I.e. map each label to fo from 0, not 2 or whatever it starts from.\n",
    "def label_pipeline(label):\n",
    "    mapping = {1:0,2:1,3:2,4:3}\n",
    "    return mapping[label]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6734",
   "metadata": {},
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ff479986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we loop through batches, this function gets applied to each raw batch.\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (label,text) in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3}\n",
    "        # Append the label to the label_list.\n",
    "        label_list.append(label_pipeline(label))\n",
    "                \n",
    "        # Return a list of ints.\n",
    "        # Get a torch tensor of the sentence, this sould be a tensor of torch.int64.\n",
    "        processed_text = torch.tensor(text_pipeline(text),dtype=torch.int64)\n",
    "        text_list.append(processed_text.clone().detach())\n",
    "    \n",
    "    # Transform the label_list into a tensor. \n",
    "    label_list = torch.tensor(label_list)\n",
    "    \n",
    "    # Pad the list of text_list tensors so they all have the same length.\n",
    "    # Use batch_first = True.\n",
    "    # Use padding_valid = PADDING_VALUE\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PADDING_VALUE)\n",
    "            \n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1287d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fcf425",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e617ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4 \n"
     ]
    }
   ],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "# Get the number of classes.\n",
    "num_class = 4\n",
    "# What are the classes? -> [1,2,3,4] corresponding to [World,Sports,Business,Sci/Tech]\n",
    "print(f\"The number of classes is {num_class} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8a40d",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dc51c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complicated model. We'll explore this after we learn word embeddings.\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_class,\n",
    "        initialize_with_glove = True,\n",
    "        fine_tune_embeddings = True\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # Set to an embedding of (vocab_size, embed_dim) size.\n",
    "        # Use padding_idx = PADDING_IDX.\n",
    "        # This is so we don't get gradients for padding tokens and use 0 as the vector for these.\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=PADDING_IDX\n",
    "        )\n",
    "        \n",
    "        if initialize_with_glove:\n",
    "            # Turn off the gradient for the embedding weight as we are going to modify it. \n",
    "            self.embedding.weight.requires_grad_(False)\n",
    "            for i in range(vocab_size):\n",
    "                # Get the token index in VOCAB.\n",
    "                token = itos[i]\n",
    "                \n",
    "                # Modify the embedding matrix to be the GloVe vector for this token.\n",
    "                self.embedding.weight[i, :] = GLOVE.get_vecs_by_tokens(token, lower_case_backup=True)\n",
    "            # Turn on the gradient after we modify it.\n",
    "            # You could do this in another way by wrapping this in @torch.no_grad decorator.\n",
    "            self.embedding.weight.requires_grad_(True)\n",
    "        \n",
    "        # No fine tuning means once you intialize, these are constant.\n",
    "        if not fine_tune_embeddings:\n",
    "            # Turn off the gradient for the embedding weight matric if you don't fine tune them.\n",
    "            self.embedding.weight.requires_grad_(False)\n",
    "        \n",
    "        \n",
    "        # Set fc to be a linear layer of dimension (embed_dim, num_class).\n",
    "        self.fc = nn.Linear(embed_dim,num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Get the embeddings for all tokens in the batch of text.\n",
    "        embedded = self.embedding(text)\n",
    "        # Across dimension 1, get the mean vector. This gets the mean vector per sentence in the batch.\n",
    "        # Make sure you squeeze any dimension that's 1. This should be (N, d), where N is the batch dimension and d is the word vector dimension.\n",
    "        embedded_sum = torch.sum(embedded, dim=1)\n",
    "        # Get the number of non-padding tokens in each sentence\n",
    "        lengths = torch.sum(text != PADDING_IDX, dim=1).unsqueeze(1)\n",
    "        \n",
    "        # Divide the sum of embeddings by the number of non-padding tokens to get the mean vector\n",
    "        embedded_mean = embedded_sum / lengths\n",
    "        # Run through a linear layer self.fc and also apply ReLU.\n",
    "        \n",
    "        logits = self.fc(F.relu(embedded_sum))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6ed5",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cef585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to be the CrossEntropyLoss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Set model to the TextClassification model.\n",
    "# Turn on intialize_with_glove ad fine_tune_embeddings.\n",
    "model = TextClassificationModel(vocab_size=len(vocab),\n",
    "                                embed_dim=EMBED_DIM,\n",
    "                                num_class=num_class,\n",
    "                                initialize_with_glove=True,\n",
    "                                fine_tune_embeddings=True)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26266d8a",
   "metadata": {},
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9c0aebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "# This puts things in a nice format.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Set num_train to 95% length of train_dataset.\n",
    "# This should be an integer.\n",
    "num_train = int(len(train_dataset)*0.95)\n",
    "num_val = len(train_dataset)-num_train\n",
    "# The array below should have 2 ints in it, num_train, and the 5% left over for validation.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train,num_val])\n",
    "\n",
    "# Set to a DataLoader on the training data with batch_size BATCH_SIZE and specify collate_batch.\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86476e2a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "24950481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 200\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(text)\n",
    "                \n",
    "        # Get the loss.\n",
    "        # need to check what label looks like \n",
    "        loss = criterion(logits,label)\n",
    "        \n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients at 0.1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the accuracy for this batch.\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        total_acc += (predicted_labels == label).sum().item()\n",
    "        # Get the number of rows in this batch. Use labels.\n",
    "        total_count += len(label)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count)\n",
    "            )\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "39a702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            logits = model(text)\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "            total_acc += (predicted_labels == label).sum().item()\n",
    "            total_count += len(label)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6fd68-891d-4d15-aedb-a2714a1080b8",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "How do you do if you do not initialize with GloVe and use fine tuning so that you do optimize the embeddings?\n",
    "- the learning is long and the model perform quite badly compared to the others -> test acc =  0.688\n",
    "\n",
    "How do you do if you initialize with GloVe but turn off fine tuning so the embedding layer is static, it does not get optimized but you put in some good embeddings to start.\n",
    "- the learning is super fast but the model do a little bit worse than the one with fineturning -> test acc = 0.865\n",
    "\n",
    "How do you do if you initialize with GloVe and you turn on fine tuning?\n",
    "- the learning is long but the results are pretty good -> test acc = 0.890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a9e02c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 7125 batches | accuracy    0.565\n",
      "| epoch   1 |   400/ 7125 batches | accuracy    0.705\n",
      "| epoch   1 |   600/ 7125 batches | accuracy    0.759\n",
      "| epoch   1 |   800/ 7125 batches | accuracy    0.769\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.776\n",
      "| epoch   1 |  1200/ 7125 batches | accuracy    0.799\n",
      "| epoch   1 |  1400/ 7125 batches | accuracy    0.798\n",
      "| epoch   1 |  1600/ 7125 batches | accuracy    0.802\n",
      "| epoch   1 |  1800/ 7125 batches | accuracy    0.802\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.801\n",
      "| epoch   1 |  2200/ 7125 batches | accuracy    0.833\n",
      "| epoch   1 |  2400/ 7125 batches | accuracy    0.812\n",
      "| epoch   1 |  2600/ 7125 batches | accuracy    0.827\n",
      "| epoch   1 |  2800/ 7125 batches | accuracy    0.838\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.821\n",
      "| epoch   1 |  3200/ 7125 batches | accuracy    0.824\n",
      "| epoch   1 |  3400/ 7125 batches | accuracy    0.824\n",
      "| epoch   1 |  3600/ 7125 batches | accuracy    0.833\n",
      "| epoch   1 |  3800/ 7125 batches | accuracy    0.828\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.838\n",
      "| epoch   1 |  4200/ 7125 batches | accuracy    0.824\n",
      "| epoch   1 |  4400/ 7125 batches | accuracy    0.832\n",
      "| epoch   1 |  4600/ 7125 batches | accuracy    0.838\n",
      "| epoch   1 |  4800/ 7125 batches | accuracy    0.844\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.845\n",
      "| epoch   1 |  5200/ 7125 batches | accuracy    0.835\n",
      "| epoch   1 |  5400/ 7125 batches | accuracy    0.847\n",
      "| epoch   1 |  5600/ 7125 batches | accuracy    0.829\n",
      "| epoch   1 |  5800/ 7125 batches | accuracy    0.833\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.858\n",
      "| epoch   1 |  6200/ 7125 batches | accuracy    0.846\n",
      "| epoch   1 |  6400/ 7125 batches | accuracy    0.841\n",
      "| epoch   1 |  6600/ 7125 batches | accuracy    0.852\n",
      "| epoch   1 |  6800/ 7125 batches | accuracy    0.851\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.856\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 201.11s | valid accuracy    0.867 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 7125 batches | accuracy    0.892\n",
      "| epoch   2 |   400/ 7125 batches | accuracy    0.902\n",
      "| epoch   2 |   600/ 7125 batches | accuracy    0.889\n",
      "| epoch   2 |   800/ 7125 batches | accuracy    0.900\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.889\n",
      "| epoch   2 |  1200/ 7125 batches | accuracy    0.893\n",
      "| epoch   2 |  1400/ 7125 batches | accuracy    0.897\n",
      "| epoch   2 |  1600/ 7125 batches | accuracy    0.902\n",
      "| epoch   2 |  1800/ 7125 batches | accuracy    0.903\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.895\n",
      "| epoch   2 |  2200/ 7125 batches | accuracy    0.890\n",
      "| epoch   2 |  2400/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  2600/ 7125 batches | accuracy    0.886\n",
      "| epoch   2 |  2800/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.901\n",
      "| epoch   2 |  3200/ 7125 batches | accuracy    0.897\n",
      "| epoch   2 |  3400/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  3600/ 7125 batches | accuracy    0.895\n",
      "| epoch   2 |  3800/ 7125 batches | accuracy    0.901\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.892\n",
      "| epoch   2 |  4200/ 7125 batches | accuracy    0.908\n",
      "| epoch   2 |  4400/ 7125 batches | accuracy    0.902\n",
      "| epoch   2 |  4600/ 7125 batches | accuracy    0.910\n",
      "| epoch   2 |  4800/ 7125 batches | accuracy    0.897\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.904\n",
      "| epoch   2 |  5200/ 7125 batches | accuracy    0.895\n",
      "| epoch   2 |  5400/ 7125 batches | accuracy    0.897\n",
      "| epoch   2 |  5600/ 7125 batches | accuracy    0.899\n",
      "| epoch   2 |  5800/ 7125 batches | accuracy    0.892\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.901\n",
      "| epoch   2 |  6200/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  6400/ 7125 batches | accuracy    0.894\n",
      "| epoch   2 |  6600/ 7125 batches | accuracy    0.897\n",
      "| epoch   2 |  6800/ 7125 batches | accuracy    0.893\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.897\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 217.32s | valid accuracy    0.881 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 7125 batches | accuracy    0.901\n",
      "| epoch   3 |   400/ 7125 batches | accuracy    0.913\n",
      "| epoch   3 |   600/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |   800/ 7125 batches | accuracy    0.917\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.899\n",
      "| epoch   3 |  1200/ 7125 batches | accuracy    0.893\n",
      "| epoch   3 |  1400/ 7125 batches | accuracy    0.901\n",
      "| epoch   3 |  1600/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  1800/ 7125 batches | accuracy    0.904\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.918\n",
      "| epoch   3 |  2200/ 7125 batches | accuracy    0.914\n",
      "| epoch   3 |  2400/ 7125 batches | accuracy    0.909\n",
      "| epoch   3 |  2600/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  2800/ 7125 batches | accuracy    0.903\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.911\n",
      "| epoch   3 |  3200/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  3400/ 7125 batches | accuracy    0.912\n",
      "| epoch   3 |  3600/ 7125 batches | accuracy    0.912\n",
      "| epoch   3 |  3800/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  4200/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  4400/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  4600/ 7125 batches | accuracy    0.901\n",
      "| epoch   3 |  4800/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.912\n",
      "| epoch   3 |  5200/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  5400/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  5600/ 7125 batches | accuracy    0.914\n",
      "| epoch   3 |  5800/ 7125 batches | accuracy    0.914\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.906\n",
      "| epoch   3 |  6200/ 7125 batches | accuracy    0.902\n",
      "| epoch   3 |  6400/ 7125 batches | accuracy    0.906\n",
      "| epoch   3 |  6600/ 7125 batches | accuracy    0.903\n",
      "| epoch   3 |  6800/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 216.38s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 7125 batches | accuracy    0.905\n",
      "| epoch   4 |   400/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |   600/ 7125 batches | accuracy    0.914\n",
      "| epoch   4 |   800/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.915\n",
      "| epoch   4 |  1200/ 7125 batches | accuracy    0.918\n",
      "| epoch   4 |  1400/ 7125 batches | accuracy    0.903\n",
      "| epoch   4 |  1600/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  1800/ 7125 batches | accuracy    0.912\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |  2200/ 7125 batches | accuracy    0.915\n",
      "| epoch   4 |  2400/ 7125 batches | accuracy    0.913\n",
      "| epoch   4 |  2600/ 7125 batches | accuracy    0.914\n",
      "| epoch   4 |  2800/ 7125 batches | accuracy    0.906\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.904\n",
      "| epoch   4 |  3200/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  3400/ 7125 batches | accuracy    0.906\n",
      "| epoch   4 |  3600/ 7125 batches | accuracy    0.905\n",
      "| epoch   4 |  3800/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.905\n",
      "| epoch   4 |  4200/ 7125 batches | accuracy    0.920\n",
      "| epoch   4 |  4400/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |  4600/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  4800/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.913\n",
      "| epoch   4 |  5200/ 7125 batches | accuracy    0.906\n",
      "| epoch   4 |  5400/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  5600/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  5800/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.912\n",
      "| epoch   4 |  6200/ 7125 batches | accuracy    0.904\n",
      "| epoch   4 |  6400/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  6600/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  6800/ 7125 batches | accuracy    0.913\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.903\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 212.40s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 7125 batches | accuracy    0.906\n",
      "| epoch   5 |   400/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |   600/ 7125 batches | accuracy    0.914\n",
      "| epoch   5 |   800/ 7125 batches | accuracy    0.908\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.912\n",
      "| epoch   5 |  1200/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  1400/ 7125 batches | accuracy    0.913\n",
      "| epoch   5 |  1600/ 7125 batches | accuracy    0.915\n",
      "| epoch   5 |  1800/ 7125 batches | accuracy    0.902\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.910\n",
      "| epoch   5 |  2200/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  2400/ 7125 batches | accuracy    0.917\n",
      "| epoch   5 |  2600/ 7125 batches | accuracy    0.915\n",
      "| epoch   5 |  2800/ 7125 batches | accuracy    0.914\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.901\n",
      "| epoch   5 |  3200/ 7125 batches | accuracy    0.904\n",
      "| epoch   5 |  3400/ 7125 batches | accuracy    0.915\n",
      "| epoch   5 |  3600/ 7125 batches | accuracy    0.904\n",
      "| epoch   5 |  3800/ 7125 batches | accuracy    0.921\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.906\n",
      "| epoch   5 |  4200/ 7125 batches | accuracy    0.914\n",
      "| epoch   5 |  4400/ 7125 batches | accuracy    0.914\n",
      "| epoch   5 |  4600/ 7125 batches | accuracy    0.913\n",
      "| epoch   5 |  4800/ 7125 batches | accuracy    0.911\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.911\n",
      "| epoch   5 |  5200/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |  5400/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  5600/ 7125 batches | accuracy    0.900\n",
      "| epoch   5 |  5800/ 7125 batches | accuracy    0.911\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.911\n",
      "| epoch   5 |  6200/ 7125 batches | accuracy    0.912\n",
      "| epoch   5 |  6400/ 7125 batches | accuracy    0.908\n",
      "| epoch   5 |  6600/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  6800/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.903\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 212.90s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.890\n"
     ]
    }
   ],
   "source": [
    "model = TextClassificationModel(vocab_size=len(vocab),\n",
    "                                embed_dim=EMBED_DIM,\n",
    "                                num_class=num_class,\n",
    "                                initialize_with_glove=True,\n",
    "                                fine_tune_embeddings=True)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b5e2fa1-5155-47d7-a249-76f24b084b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 7125 batches | accuracy    0.547\n",
      "| epoch   1 |   400/ 7125 batches | accuracy    0.696\n",
      "| epoch   1 |   600/ 7125 batches | accuracy    0.733\n",
      "| epoch   1 |   800/ 7125 batches | accuracy    0.756\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.762\n",
      "| epoch   1 |  1200/ 7125 batches | accuracy    0.757\n",
      "| epoch   1 |  1400/ 7125 batches | accuracy    0.773\n",
      "| epoch   1 |  1600/ 7125 batches | accuracy    0.773\n",
      "| epoch   1 |  1800/ 7125 batches | accuracy    0.781\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.774\n",
      "| epoch   1 |  2200/ 7125 batches | accuracy    0.775\n",
      "| epoch   1 |  2400/ 7125 batches | accuracy    0.778\n",
      "| epoch   1 |  2600/ 7125 batches | accuracy    0.773\n",
      "| epoch   1 |  2800/ 7125 batches | accuracy    0.800\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.793\n",
      "| epoch   1 |  3200/ 7125 batches | accuracy    0.794\n",
      "| epoch   1 |  3400/ 7125 batches | accuracy    0.796\n",
      "| epoch   1 |  3600/ 7125 batches | accuracy    0.773\n",
      "| epoch   1 |  3800/ 7125 batches | accuracy    0.783\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.789\n",
      "| epoch   1 |  4200/ 7125 batches | accuracy    0.800\n",
      "| epoch   1 |  4400/ 7125 batches | accuracy    0.799\n",
      "| epoch   1 |  4600/ 7125 batches | accuracy    0.789\n",
      "| epoch   1 |  4800/ 7125 batches | accuracy    0.799\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.797\n",
      "| epoch   1 |  5200/ 7125 batches | accuracy    0.793\n",
      "| epoch   1 |  5400/ 7125 batches | accuracy    0.795\n",
      "| epoch   1 |  5600/ 7125 batches | accuracy    0.792\n",
      "| epoch   1 |  5800/ 7125 batches | accuracy    0.787\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.796\n",
      "| epoch   1 |  6200/ 7125 batches | accuracy    0.794\n",
      "| epoch   1 |  6400/ 7125 batches | accuracy    0.783\n",
      "| epoch   1 |  6600/ 7125 batches | accuracy    0.801\n",
      "| epoch   1 |  6800/ 7125 batches | accuracy    0.809\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.797\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  8.45s | valid accuracy    0.790 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 7125 batches | accuracy    0.874\n",
      "| epoch   2 |   400/ 7125 batches | accuracy    0.871\n",
      "| epoch   2 |   600/ 7125 batches | accuracy    0.879\n",
      "| epoch   2 |   800/ 7125 batches | accuracy    0.869\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.876\n",
      "| epoch   2 |  1200/ 7125 batches | accuracy    0.864\n",
      "| epoch   2 |  1400/ 7125 batches | accuracy    0.867\n",
      "| epoch   2 |  1600/ 7125 batches | accuracy    0.884\n",
      "| epoch   2 |  1800/ 7125 batches | accuracy    0.867\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.863\n",
      "| epoch   2 |  2200/ 7125 batches | accuracy    0.877\n",
      "| epoch   2 |  2400/ 7125 batches | accuracy    0.867\n",
      "| epoch   2 |  2600/ 7125 batches | accuracy    0.874\n",
      "| epoch   2 |  2800/ 7125 batches | accuracy    0.875\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.868\n",
      "| epoch   2 |  3200/ 7125 batches | accuracy    0.883\n",
      "| epoch   2 |  3400/ 7125 batches | accuracy    0.861\n",
      "| epoch   2 |  3600/ 7125 batches | accuracy    0.870\n",
      "| epoch   2 |  3800/ 7125 batches | accuracy    0.871\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.870\n",
      "| epoch   2 |  4200/ 7125 batches | accuracy    0.871\n",
      "| epoch   2 |  4400/ 7125 batches | accuracy    0.866\n",
      "| epoch   2 |  4600/ 7125 batches | accuracy    0.873\n",
      "| epoch   2 |  4800/ 7125 batches | accuracy    0.867\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.868\n",
      "| epoch   2 |  5200/ 7125 batches | accuracy    0.863\n",
      "| epoch   2 |  5400/ 7125 batches | accuracy    0.856\n",
      "| epoch   2 |  5600/ 7125 batches | accuracy    0.863\n",
      "| epoch   2 |  5800/ 7125 batches | accuracy    0.865\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.854\n",
      "| epoch   2 |  6200/ 7125 batches | accuracy    0.858\n",
      "| epoch   2 |  6400/ 7125 batches | accuracy    0.865\n",
      "| epoch   2 |  6600/ 7125 batches | accuracy    0.866\n",
      "| epoch   2 |  6800/ 7125 batches | accuracy    0.859\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.858\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.81s | valid accuracy    0.865 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 7125 batches | accuracy    0.867\n",
      "| epoch   3 |   400/ 7125 batches | accuracy    0.873\n",
      "| epoch   3 |   600/ 7125 batches | accuracy    0.877\n",
      "| epoch   3 |   800/ 7125 batches | accuracy    0.874\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.883\n",
      "| epoch   3 |  1200/ 7125 batches | accuracy    0.868\n",
      "| epoch   3 |  1400/ 7125 batches | accuracy    0.876\n",
      "| epoch   3 |  1600/ 7125 batches | accuracy    0.876\n",
      "| epoch   3 |  1800/ 7125 batches | accuracy    0.867\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.883\n",
      "| epoch   3 |  2200/ 7125 batches | accuracy    0.877\n",
      "| epoch   3 |  2400/ 7125 batches | accuracy    0.864\n",
      "| epoch   3 |  2600/ 7125 batches | accuracy    0.881\n",
      "| epoch   3 |  2800/ 7125 batches | accuracy    0.882\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.879\n",
      "| epoch   3 |  3200/ 7125 batches | accuracy    0.870\n",
      "| epoch   3 |  3400/ 7125 batches | accuracy    0.869\n",
      "| epoch   3 |  3600/ 7125 batches | accuracy    0.876\n",
      "| epoch   3 |  3800/ 7125 batches | accuracy    0.861\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.887\n",
      "| epoch   3 |  4200/ 7125 batches | accuracy    0.867\n",
      "| epoch   3 |  4400/ 7125 batches | accuracy    0.865\n",
      "| epoch   3 |  4600/ 7125 batches | accuracy    0.872\n",
      "| epoch   3 |  4800/ 7125 batches | accuracy    0.879\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.874\n",
      "| epoch   3 |  5200/ 7125 batches | accuracy    0.870\n",
      "| epoch   3 |  5400/ 7125 batches | accuracy    0.866\n",
      "| epoch   3 |  5600/ 7125 batches | accuracy    0.859\n",
      "| epoch   3 |  5800/ 7125 batches | accuracy    0.870\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.870\n",
      "| epoch   3 |  6200/ 7125 batches | accuracy    0.878\n",
      "| epoch   3 |  6400/ 7125 batches | accuracy    0.862\n",
      "| epoch   3 |  6600/ 7125 batches | accuracy    0.871\n",
      "| epoch   3 |  6800/ 7125 batches | accuracy    0.871\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.871\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  8.05s | valid accuracy    0.865 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 7125 batches | accuracy    0.870\n",
      "| epoch   4 |   400/ 7125 batches | accuracy    0.865\n",
      "| epoch   4 |   600/ 7125 batches | accuracy    0.874\n",
      "| epoch   4 |   800/ 7125 batches | accuracy    0.862\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.867\n",
      "| epoch   4 |  1200/ 7125 batches | accuracy    0.866\n",
      "| epoch   4 |  1400/ 7125 batches | accuracy    0.865\n",
      "| epoch   4 |  1600/ 7125 batches | accuracy    0.874\n",
      "| epoch   4 |  1800/ 7125 batches | accuracy    0.868\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.866\n",
      "| epoch   4 |  2200/ 7125 batches | accuracy    0.866\n",
      "| epoch   4 |  2400/ 7125 batches | accuracy    0.870\n",
      "| epoch   4 |  2600/ 7125 batches | accuracy    0.879\n",
      "| epoch   4 |  2800/ 7125 batches | accuracy    0.881\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.876\n",
      "| epoch   4 |  3200/ 7125 batches | accuracy    0.867\n",
      "| epoch   4 |  3400/ 7125 batches | accuracy    0.876\n",
      "| epoch   4 |  3600/ 7125 batches | accuracy    0.875\n",
      "| epoch   4 |  3800/ 7125 batches | accuracy    0.875\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.879\n",
      "| epoch   4 |  4200/ 7125 batches | accuracy    0.871\n",
      "| epoch   4 |  4400/ 7125 batches | accuracy    0.870\n",
      "| epoch   4 |  4600/ 7125 batches | accuracy    0.877\n",
      "| epoch   4 |  4800/ 7125 batches | accuracy    0.871\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.873\n",
      "| epoch   4 |  5200/ 7125 batches | accuracy    0.873\n",
      "| epoch   4 |  5400/ 7125 batches | accuracy    0.877\n",
      "| epoch   4 |  5600/ 7125 batches | accuracy    0.867\n",
      "| epoch   4 |  5800/ 7125 batches | accuracy    0.868\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.877\n",
      "| epoch   4 |  6200/ 7125 batches | accuracy    0.878\n",
      "| epoch   4 |  6400/ 7125 batches | accuracy    0.872\n",
      "| epoch   4 |  6600/ 7125 batches | accuracy    0.874\n",
      "| epoch   4 |  6800/ 7125 batches | accuracy    0.883\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.873\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  8.79s | valid accuracy    0.869 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 7125 batches | accuracy    0.860\n",
      "| epoch   5 |   400/ 7125 batches | accuracy    0.878\n",
      "| epoch   5 |   600/ 7125 batches | accuracy    0.862\n",
      "| epoch   5 |   800/ 7125 batches | accuracy    0.870\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.880\n",
      "| epoch   5 |  1200/ 7125 batches | accuracy    0.868\n",
      "| epoch   5 |  1400/ 7125 batches | accuracy    0.874\n",
      "| epoch   5 |  1600/ 7125 batches | accuracy    0.873\n",
      "| epoch   5 |  1800/ 7125 batches | accuracy    0.864\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.874\n",
      "| epoch   5 |  2200/ 7125 batches | accuracy    0.876\n",
      "| epoch   5 |  2400/ 7125 batches | accuracy    0.862\n",
      "| epoch   5 |  2600/ 7125 batches | accuracy    0.872\n",
      "| epoch   5 |  2800/ 7125 batches | accuracy    0.869\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.877\n",
      "| epoch   5 |  3200/ 7125 batches | accuracy    0.861\n",
      "| epoch   5 |  3400/ 7125 batches | accuracy    0.872\n",
      "| epoch   5 |  3600/ 7125 batches | accuracy    0.875\n",
      "| epoch   5 |  3800/ 7125 batches | accuracy    0.875\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.871\n",
      "| epoch   5 |  4200/ 7125 batches | accuracy    0.873\n",
      "| epoch   5 |  4400/ 7125 batches | accuracy    0.877\n",
      "| epoch   5 |  4600/ 7125 batches | accuracy    0.873\n",
      "| epoch   5 |  4800/ 7125 batches | accuracy    0.865\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.877\n",
      "| epoch   5 |  5200/ 7125 batches | accuracy    0.881\n",
      "| epoch   5 |  5400/ 7125 batches | accuracy    0.878\n",
      "| epoch   5 |  5600/ 7125 batches | accuracy    0.877\n",
      "| epoch   5 |  5800/ 7125 batches | accuracy    0.868\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.891\n",
      "| epoch   5 |  6200/ 7125 batches | accuracy    0.881\n",
      "| epoch   5 |  6400/ 7125 batches | accuracy    0.866\n",
      "| epoch   5 |  6600/ 7125 batches | accuracy    0.870\n",
      "| epoch   5 |  6800/ 7125 batches | accuracy    0.872\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.867\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  7.95s | valid accuracy    0.870 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.865\n"
     ]
    }
   ],
   "source": [
    "model = TextClassificationModel(vocab_size=len(vocab),\n",
    "                                embed_dim=EMBED_DIM,\n",
    "                                num_class=num_class,\n",
    "                                initialize_with_glove=True,\n",
    "                                fine_tune_embeddings=False)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c010db71-4110-4a9b-a200-a8a97b2e3fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 7125 batches | accuracy    0.377\n",
      "| epoch   1 |   400/ 7125 batches | accuracy    0.478\n",
      "| epoch   1 |   600/ 7125 batches | accuracy    0.488\n",
      "| epoch   1 |   800/ 7125 batches | accuracy    0.492\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.509\n",
      "| epoch   1 |  1200/ 7125 batches | accuracy    0.500\n",
      "| epoch   1 |  1400/ 7125 batches | accuracy    0.502\n",
      "| epoch   1 |  1600/ 7125 batches | accuracy    0.484\n",
      "| epoch   1 |  1800/ 7125 batches | accuracy    0.507\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.501\n",
      "| epoch   1 |  2200/ 7125 batches | accuracy    0.529\n",
      "| epoch   1 |  2400/ 7125 batches | accuracy    0.507\n",
      "| epoch   1 |  2600/ 7125 batches | accuracy    0.522\n",
      "| epoch   1 |  2800/ 7125 batches | accuracy    0.533\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.502\n",
      "| epoch   1 |  3200/ 7125 batches | accuracy    0.523\n",
      "| epoch   1 |  3400/ 7125 batches | accuracy    0.532\n",
      "| epoch   1 |  3600/ 7125 batches | accuracy    0.533\n",
      "| epoch   1 |  3800/ 7125 batches | accuracy    0.542\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.539\n",
      "| epoch   1 |  4200/ 7125 batches | accuracy    0.550\n",
      "| epoch   1 |  4400/ 7125 batches | accuracy    0.538\n",
      "| epoch   1 |  4600/ 7125 batches | accuracy    0.561\n",
      "| epoch   1 |  4800/ 7125 batches | accuracy    0.544\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.556\n",
      "| epoch   1 |  5200/ 7125 batches | accuracy    0.541\n",
      "| epoch   1 |  5400/ 7125 batches | accuracy    0.558\n",
      "| epoch   1 |  5600/ 7125 batches | accuracy    0.553\n",
      "| epoch   1 |  5800/ 7125 batches | accuracy    0.565\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.559\n",
      "| epoch   1 |  6200/ 7125 batches | accuracy    0.576\n",
      "| epoch   1 |  6400/ 7125 batches | accuracy    0.578\n",
      "| epoch   1 |  6600/ 7125 batches | accuracy    0.569\n",
      "| epoch   1 |  6800/ 7125 batches | accuracy    0.558\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.550\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 208.92s | valid accuracy    0.526 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 7125 batches | accuracy    0.614\n",
      "| epoch   2 |   400/ 7125 batches | accuracy    0.631\n",
      "| epoch   2 |   600/ 7125 batches | accuracy    0.634\n",
      "| epoch   2 |   800/ 7125 batches | accuracy    0.651\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.647\n",
      "| epoch   2 |  1200/ 7125 batches | accuracy    0.642\n",
      "| epoch   2 |  1400/ 7125 batches | accuracy    0.633\n",
      "| epoch   2 |  1600/ 7125 batches | accuracy    0.629\n",
      "| epoch   2 |  1800/ 7125 batches | accuracy    0.665\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.633\n",
      "| epoch   2 |  2200/ 7125 batches | accuracy    0.650\n",
      "| epoch   2 |  2400/ 7125 batches | accuracy    0.653\n",
      "| epoch   2 |  2600/ 7125 batches | accuracy    0.662\n",
      "| epoch   2 |  2800/ 7125 batches | accuracy    0.651\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.669\n",
      "| epoch   2 |  3200/ 7125 batches | accuracy    0.631\n",
      "| epoch   2 |  3400/ 7125 batches | accuracy    0.633\n",
      "| epoch   2 |  3600/ 7125 batches | accuracy    0.647\n",
      "| epoch   2 |  3800/ 7125 batches | accuracy    0.646\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.647\n",
      "| epoch   2 |  4200/ 7125 batches | accuracy    0.642\n",
      "| epoch   2 |  4400/ 7125 batches | accuracy    0.646\n",
      "| epoch   2 |  4600/ 7125 batches | accuracy    0.655\n",
      "| epoch   2 |  4800/ 7125 batches | accuracy    0.642\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.650\n",
      "| epoch   2 |  5200/ 7125 batches | accuracy    0.646\n",
      "| epoch   2 |  5400/ 7125 batches | accuracy    0.662\n",
      "| epoch   2 |  5600/ 7125 batches | accuracy    0.642\n",
      "| epoch   2 |  5800/ 7125 batches | accuracy    0.662\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.657\n",
      "| epoch   2 |  6200/ 7125 batches | accuracy    0.643\n",
      "| epoch   2 |  6400/ 7125 batches | accuracy    0.650\n",
      "| epoch   2 |  6600/ 7125 batches | accuracy    0.647\n",
      "| epoch   2 |  6800/ 7125 batches | accuracy    0.675\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.644\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 206.49s | valid accuracy    0.663 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 7125 batches | accuracy    0.669\n",
      "| epoch   3 |   400/ 7125 batches | accuracy    0.674\n",
      "| epoch   3 |   600/ 7125 batches | accuracy    0.650\n",
      "| epoch   3 |   800/ 7125 batches | accuracy    0.691\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.680\n",
      "| epoch   3 |  1200/ 7125 batches | accuracy    0.683\n",
      "| epoch   3 |  1400/ 7125 batches | accuracy    0.682\n",
      "| epoch   3 |  1600/ 7125 batches | accuracy    0.678\n",
      "| epoch   3 |  1800/ 7125 batches | accuracy    0.682\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.667\n",
      "| epoch   3 |  2200/ 7125 batches | accuracy    0.685\n",
      "| epoch   3 |  2400/ 7125 batches | accuracy    0.677\n",
      "| epoch   3 |  2600/ 7125 batches | accuracy    0.694\n",
      "| epoch   3 |  2800/ 7125 batches | accuracy    0.672\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.697\n",
      "| epoch   3 |  3200/ 7125 batches | accuracy    0.681\n",
      "| epoch   3 |  3400/ 7125 batches | accuracy    0.675\n",
      "| epoch   3 |  3600/ 7125 batches | accuracy    0.698\n",
      "| epoch   3 |  3800/ 7125 batches | accuracy    0.688\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.678\n",
      "| epoch   3 |  4200/ 7125 batches | accuracy    0.685\n",
      "| epoch   3 |  4400/ 7125 batches | accuracy    0.677\n",
      "| epoch   3 |  4600/ 7125 batches | accuracy    0.693\n",
      "| epoch   3 |  4800/ 7125 batches | accuracy    0.687\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.672\n",
      "| epoch   3 |  5200/ 7125 batches | accuracy    0.694\n",
      "| epoch   3 |  5400/ 7125 batches | accuracy    0.675\n",
      "| epoch   3 |  5600/ 7125 batches | accuracy    0.678\n",
      "| epoch   3 |  5800/ 7125 batches | accuracy    0.682\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.684\n",
      "| epoch   3 |  6200/ 7125 batches | accuracy    0.684\n",
      "| epoch   3 |  6400/ 7125 batches | accuracy    0.684\n",
      "| epoch   3 |  6600/ 7125 batches | accuracy    0.688\n",
      "| epoch   3 |  6800/ 7125 batches | accuracy    0.699\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.671\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 211.98s | valid accuracy    0.690 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 7125 batches | accuracy    0.707\n",
      "| epoch   4 |   400/ 7125 batches | accuracy    0.702\n",
      "| epoch   4 |   600/ 7125 batches | accuracy    0.685\n",
      "| epoch   4 |   800/ 7125 batches | accuracy    0.687\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.700\n",
      "| epoch   4 |  1200/ 7125 batches | accuracy    0.688\n",
      "| epoch   4 |  1400/ 7125 batches | accuracy    0.687\n",
      "| epoch   4 |  1600/ 7125 batches | accuracy    0.692\n",
      "| epoch   4 |  1800/ 7125 batches | accuracy    0.680\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.699\n",
      "| epoch   4 |  2200/ 7125 batches | accuracy    0.693\n",
      "| epoch   4 |  2400/ 7125 batches | accuracy    0.677\n",
      "| epoch   4 |  2600/ 7125 batches | accuracy    0.710\n",
      "| epoch   4 |  2800/ 7125 batches | accuracy    0.688\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.679\n",
      "| epoch   4 |  3200/ 7125 batches | accuracy    0.683\n",
      "| epoch   4 |  3400/ 7125 batches | accuracy    0.686\n",
      "| epoch   4 |  3600/ 7125 batches | accuracy    0.694\n",
      "| epoch   4 |  3800/ 7125 batches | accuracy    0.690\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.678\n",
      "| epoch   4 |  4200/ 7125 batches | accuracy    0.682\n",
      "| epoch   4 |  4400/ 7125 batches | accuracy    0.689\n",
      "| epoch   4 |  4600/ 7125 batches | accuracy    0.688\n",
      "| epoch   4 |  4800/ 7125 batches | accuracy    0.686\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.698\n",
      "| epoch   4 |  5200/ 7125 batches | accuracy    0.682\n",
      "| epoch   4 |  5400/ 7125 batches | accuracy    0.672\n",
      "| epoch   4 |  5600/ 7125 batches | accuracy    0.693\n",
      "| epoch   4 |  5800/ 7125 batches | accuracy    0.686\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.687\n",
      "| epoch   4 |  6200/ 7125 batches | accuracy    0.697\n",
      "| epoch   4 |  6400/ 7125 batches | accuracy    0.690\n",
      "| epoch   4 |  6600/ 7125 batches | accuracy    0.683\n",
      "| epoch   4 |  6800/ 7125 batches | accuracy    0.703\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 211.55s | valid accuracy    0.693 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 7125 batches | accuracy    0.715\n",
      "| epoch   5 |   400/ 7125 batches | accuracy    0.689\n",
      "| epoch   5 |   600/ 7125 batches | accuracy    0.681\n",
      "| epoch   5 |   800/ 7125 batches | accuracy    0.703\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.675\n",
      "| epoch   5 |  1200/ 7125 batches | accuracy    0.699\n",
      "| epoch   5 |  1400/ 7125 batches | accuracy    0.695\n",
      "| epoch   5 |  1600/ 7125 batches | accuracy    0.691\n",
      "| epoch   5 |  1800/ 7125 batches | accuracy    0.682\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.690\n",
      "| epoch   5 |  2200/ 7125 batches | accuracy    0.690\n",
      "| epoch   5 |  2400/ 7125 batches | accuracy    0.692\n",
      "| epoch   5 |  2600/ 7125 batches | accuracy    0.688\n",
      "| epoch   5 |  2800/ 7125 batches | accuracy    0.679\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.685\n",
      "| epoch   5 |  3200/ 7125 batches | accuracy    0.690\n",
      "| epoch   5 |  3400/ 7125 batches | accuracy    0.688\n",
      "| epoch   5 |  3600/ 7125 batches | accuracy    0.712\n",
      "| epoch   5 |  3800/ 7125 batches | accuracy    0.695\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.682\n",
      "| epoch   5 |  4200/ 7125 batches | accuracy    0.684\n",
      "| epoch   5 |  4400/ 7125 batches | accuracy    0.678\n",
      "| epoch   5 |  4600/ 7125 batches | accuracy    0.693\n",
      "| epoch   5 |  4800/ 7125 batches | accuracy    0.686\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.681\n",
      "| epoch   5 |  5200/ 7125 batches | accuracy    0.683\n",
      "| epoch   5 |  5400/ 7125 batches | accuracy    0.703\n",
      "| epoch   5 |  5600/ 7125 batches | accuracy    0.700\n",
      "| epoch   5 |  5800/ 7125 batches | accuracy    0.702\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.692\n",
      "| epoch   5 |  6200/ 7125 batches | accuracy    0.682\n",
      "| epoch   5 |  6400/ 7125 batches | accuracy    0.694\n",
      "| epoch   5 |  6600/ 7125 batches | accuracy    0.686\n",
      "| epoch   5 |  6800/ 7125 batches | accuracy    0.686\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.688\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 211.57s | valid accuracy    0.695 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.688\n"
     ]
    }
   ],
   "source": [
    "model = TextClassificationModel(vocab_size=len(vocab),\n",
    "                                embed_dim=EMBED_DIM,\n",
    "                                num_class=num_class,\n",
    "                                initialize_with_glove=False,\n",
    "                                fine_tune_embeddings=True)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51335c-fff3-4eb5-af41-32cce75782c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
