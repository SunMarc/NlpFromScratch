{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db00378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9140e3c",
   "metadata": {},
   "source": [
    "### Constants and configs used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9ace94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MARKER = '.'\n",
    "# N-gram level; P(w_t | w_{t-1}, ..., w_{t-n+1}).\n",
    "# We use 3 words to predict the next word.\n",
    "n = 4\n",
    "# Hidden layer dimension.\n",
    "h = 20\n",
    "# Word embedding dimension.\n",
    "m = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f16039",
   "metadata": {},
   "source": [
    "### Get the dataset and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9c4dca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, words, chars):\n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        # Inverse dictionaries mapping char tokens to unique ids and the reverse.\n",
    "        # Tokens in this case are the unique chars we passed in above.\n",
    "        # Each token should be mappend to a unique integer and MARKER should have token 0.\n",
    "        # For example, stoi should be like {'.' -> 0, 'a' -> 1, 'b' -> 2} if I pass in chars = '.ab'.\n",
    "        self.stoi = {letter : i for i,letter in enumerate(self.chars)} | {'.':0}\n",
    "        self.itos = {val:key for key,val in self.stoi.items()} # Inverse mapping.\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of words.\n",
    "        return len(self.words)\n",
    "\n",
    "    def contains(self, word):\n",
    "        # Check if word is in self.words and return True/False if it is, is not.\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        # Return the vocabulary size.\n",
    "        # not sure about that\n",
    "        return len(self.chars)\n",
    "\n",
    "    def encode(self, word):\n",
    "        # Express this word as a list of int ids. For example, maybe \".abc\" -> [0, 1, 2, 3].\n",
    "        # This assumes 'a' -> 1, etc.\n",
    "        return [self.stoi[char] for char in word]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        # For a set of tokens, return back the string.\n",
    "        # For example, maybe [1, 1, 2] -> \"aac\"\n",
    "        return ''.join([self.itos[token] for token in tokens])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This is used so we can loop over the data.\n",
    "        word = self.words[idx]\n",
    "        return self.encode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32e480ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(window, input_file = 'names.txt'):\n",
    "    \"\"\"\n",
    "    This takes a file of words and separates all the words.\n",
    "    It then gets all the characters present in the universe of words and then ouputs the statistics. \n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = f.read()\n",
    "    # Split the file by new lines. You should get a list of names.\n",
    "    words = data.split('\\n')\n",
    "    words = [ word.strip() for word in words ] # This gets rid of any trailing and starting white spaces.\n",
    "    words = [ word for word in words if len(word)>0 ] # Filter out all the empty words.\n",
    "    \n",
    "    # This gets the universe of all characters.\n",
    "    chars = set()\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            chars.add(char)\n",
    "    chars = sorted(list(chars))\n",
    "\n",
    "    # Will force chars to have MARKER having index 0.\n",
    "    chars= [MARKER] + chars\n",
    "    \n",
    "    # Pad each word with a context window of size n-1.\n",
    "    # Why? a word like \"abc\" should becomes \"..abc..\" if the window is size 3.\n",
    "    # This is some we can get pair of (x, y) data like this: \"..\" -> \"a\", \".a\" -> \"b\", \"ab\" -> \"c\", \"bc\" -> \".\", \"c.\" -> \".\"\n",
    "    # I.e. this allows us to know that \"a\" is a start character.\n",
    "    # So you should get something like [\"ab\", \"c\"] -> [\"..ab..\", \"..c..\"], for example.\n",
    "    window = '.'*(n-1)\n",
    "    words = [ f\"{window}{word}{window}\" for word in words]\n",
    "            \n",
    "    print(f\"The number of examples in the dataset: {len(words)}\")\n",
    "    print(f\"The number of unique characters in the vocabulary: {len(chars)}\")\n",
    "    print(f\"The vocabulary we have is: {''.join(chars)}\")\n",
    "\n",
    "    # Partition the input data into a training, validation, and the test set.\n",
    "    out_of_sample_set_size = min(2000, int(len(words) * 0.1)) # We use 10% of the training set, or up to 2000 examples.\n",
    "    test_set_size = 1500\n",
    "    \n",
    "    # First, get a random permutation of randomly permute of size len(words).\n",
    "    # Then, convert this to a list. \n",
    "    # This index list is used below to get the train, validation, and test sets.\n",
    "    rp = torch.randperm(len(words)).tolist()\n",
    "    \n",
    "    # Get train, validation, and test set.\n",
    "    train_words = [words[i] for i in rp[:-out_of_sample_set_size]]\n",
    "    validation_words = [words[i] for i in rp[-out_of_sample_set_size:-test_set_size]]\n",
    "    test_words = [words[i] for i in rp[-test_set_size:]]    \n",
    "    print(f\"We've split up the dataset into {len(train_words)}, {len(validation_words)}, {len(test_words)} training, validation, and test examples\")\n",
    "\n",
    "    # But the data in the data set objects.\n",
    "    train_dataset = CharDataset(train_words, chars)\n",
    "    validation_dataset = CharDataset(validation_words, chars)\n",
    "    test_dataset = CharDataset(test_words, chars)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36b97647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the dataset: 32033\n",
      "The number of unique characters in the vocabulary: 27\n",
      "The vocabulary we have is: .abcdefghijklmnopqrstuvwxyz\n",
      "We've split up the dataset into 30033, 500, 1500 training, validation, and test examples\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = create_datasets(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b45bc",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c6cb698c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...maelani...'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first word in \"train_dataset\"\n",
    "train_dataset.decode(train_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8369b489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the stoi map of train_dataset. How many keys does it have?\n",
    "len(train_dataset.stoi.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4881fe",
   "metadata": {},
   "source": [
    "### Get the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8f7967c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, window):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    # For ech word.\n",
    "    for i, word in enumerate(dataset):\n",
    "        # Grab a context of size window and window-1 characters will be in x, 1 will be in y.\n",
    "        for j, _ in enumerate(word):\n",
    "            # If there is no widow of size window left, break.\n",
    "            if j + window > len(word) - 1:\n",
    "                break\n",
    "            word_window = word[j:j + window-1]\n",
    "            x, y = word_window, word[j + window-1]\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "            \n",
    "    return DataLoader(\n",
    "        TensorDataset(torch.tensor(x_list), torch.tensor(y_list)),\n",
    "        BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5086d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_dataset, n)\n",
    "validation_dataloader = create_dataloader(validation_dataset, n)\n",
    "test_dataloader = create_dataloader(test_dataset, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca51b36",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8ab8cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the first Neural language models!\n",
    "class CharacterNeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, V, m, h, n):\n",
    "        super(CharacterNeuralLanguageModel, self).__init__()\n",
    "        \n",
    "        # Vocabulary size.\n",
    "        self.V = V\n",
    "        \n",
    "        # Embedding dimension, per word.\n",
    "        self.m = m\n",
    "        \n",
    "        # Hidden dimension.\n",
    "        self.h = h\n",
    "        \n",
    "        # N in \"N-gram\"\n",
    "        self.n = n\n",
    "        \n",
    "        # Can you change all this stuff to use nn.Linear?\n",
    "        # Ca also use nn.Parameter(torch.zeros(V, m)) for self.C but then we need one-hot and this is slow.\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        self.H = nn.Parameter(torch.zeros((n-1) * m, h))\n",
    "        self.W = nn.Parameter(torch.zeros((n-1) * m, V))\n",
    "        self.U = nn.Parameter(torch.zeros(h, V))\n",
    "        \n",
    "        self.b = torch.nn.Parameter(torch.ones(V))\n",
    "        self.d = torch.nn.Parameter(torch.ones(h))\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Intitialize C, H, W, U in a nice way. Use xavier initialization for the weights.\n",
    "        # On a first run, just pass.\n",
    "        torch.nn.init.xavier_uniform_(self.C.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.H)\n",
    "        torch.nn.init.xavier_uniform_(self.W)\n",
    "        torch.nn.init.xavier_uniform_(self.U)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is of dimenson N = batch size X n-1\n",
    "        \n",
    "        # N X (n-1) X m \n",
    "        x = self.C(x)\n",
    "        \n",
    "        # N\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # N X (n-1) * m -> concat les embeddings \n",
    "        x = x.view(B, -1)\n",
    "    \n",
    "        # N X V\n",
    "        y = self.b + torch.matmul(x, self.W) + torch.matmul(nn.Tanh()(self.d + torch.matmul(x, self.H)), self.U)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899045e6",
   "metadata": {},
   "source": [
    "### Set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "02b8e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical to lecture.\n",
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "model = CharacterNeuralLanguageModel(\n",
    "    train_dataset.get_vocab_size(), m, h, n\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler =torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5bb91",
   "metadata": {},
   "source": [
    "### Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e49ebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(total_loss, total_batches):\n",
    "    return torch.exp(torch.tensor(total_loss / total_batches)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d58cc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_batches = 0.0, 0.0\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(x)\n",
    "                        \n",
    "        # Get the loss.\n",
    "        loss = criterion(input=logits, target=y.squeeze(-1))\n",
    "\n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "                        \n",
    "        # Clip the gradients so they don't explode. Look at how this is done in lecture.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "                \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            perplexity = calculate_perplexity(total_loss,  total_batches)\n",
    "            print(\n",
    "                \"| epoch {:3d} \"\n",
    "                \"| {:5d}/{:5d} batches \"\n",
    "                \"| perplexity {:8.3f} \"\n",
    "                \"| loss {:8.3f} \"\n",
    "                .format(\n",
    "                    epoch,\n",
    "                    idx,\n",
    "                    len(dataloader),\n",
    "                    perplexity,\n",
    "                    total_loss / total_batches,\n",
    "                )\n",
    "            )\n",
    "            total_loss, total_batches = 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "85722617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_batches = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(dataloader):\n",
    "            logits = model(x)\n",
    "            total_loss += criterion(input=logits, target=y.squeeze(-1)).item()\n",
    "            total_batches += 1\n",
    "    return total_loss / total_batches, calculate_perplexity(total_loss,  total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "21ba24f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [00:00, 2581.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/15247 batches | perplexity    6.911 | loss    1.933 \n",
      "| epoch   1 |  1000/15247 batches | perplexity    7.198 | loss    1.974 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1794it [00:00, 2640.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1500/15247 batches | perplexity    6.807 | loss    1.918 \n",
      "| epoch   1 |  2000/15247 batches | perplexity    7.080 | loss    1.957 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2886it [00:01, 2717.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  2500/15247 batches | perplexity    6.921 | loss    1.935 \n",
      "| epoch   1 |  3000/15247 batches | perplexity    7.212 | loss    1.976 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [00:01, 2759.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  3500/15247 batches | perplexity    7.158 | loss    1.968 \n",
      "| epoch   1 |  4000/15247 batches | perplexity    7.093 | loss    1.959 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4828it [00:01, 2729.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  4500/15247 batches | perplexity    6.966 | loss    1.941 \n",
      "| epoch   1 |  5000/15247 batches | perplexity    7.051 | loss    1.953 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5939it [00:02, 2768.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  5500/15247 batches | perplexity    7.008 | loss    1.947 \n",
      "| epoch   1 |  6000/15247 batches | perplexity    6.832 | loss    1.922 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7063it [00:02, 2797.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  6500/15247 batches | perplexity    6.770 | loss    1.912 \n",
      "| epoch   1 |  7000/15247 batches | perplexity    6.897 | loss    1.931 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7904it [00:02, 2780.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  7500/15247 batches | perplexity    7.047 | loss    1.953 \n",
      "| epoch   1 |  8000/15247 batches | perplexity    6.766 | loss    1.912 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9006it [00:03, 2684.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  8500/15247 batches | perplexity    7.046 | loss    1.952 \n",
      "| epoch   1 |  9000/15247 batches | perplexity    6.959 | loss    1.940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9817it [00:03, 2602.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  9500/15247 batches | perplexity    7.050 | loss    1.953 \n",
      "| epoch   1 | 10000/15247 batches | perplexity    7.029 | loss    1.950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10902it [00:04, 2687.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 10500/15247 batches | perplexity    7.001 | loss    1.946 \n",
      "| epoch   1 | 11000/15247 batches | perplexity    6.833 | loss    1.922 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12002it [00:04, 2684.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 11500/15247 batches | perplexity    6.919 | loss    1.934 \n",
      "| epoch   1 | 12000/15247 batches | perplexity    6.949 | loss    1.939 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12826it [00:04, 2712.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 12500/15247 batches | perplexity    6.820 | loss    1.920 \n",
      "| epoch   1 | 13000/15247 batches | perplexity    6.985 | loss    1.944 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13935it [00:05, 2745.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 13500/15247 batches | perplexity    7.195 | loss    1.973 \n",
      "| epoch   1 | 14000/15247 batches | perplexity    6.977 | loss    1.943 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15043it [00:05, 2759.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 14500/15247 batches | perplexity    6.990 | loss    1.944 \n",
      "| epoch   1 | 15000/15247 batches | perplexity    6.968 | loss    1.941 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15247it [00:05, 2697.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  5.68s | valid perplexity    7.084 | valid loss    1.958\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [00:00, 2583.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |   500/15247 batches | perplexity    6.876 | loss    1.928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "808it [00:00, 2664.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  1000/15247 batches | perplexity    7.101 | loss    1.960 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1345it [00:00, 2649.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  1500/15247 batches | perplexity    7.041 | loss    1.952 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1887it [00:00, 2679.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  2000/15247 batches | perplexity    7.002 | loss    1.946 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2444it [00:00, 2732.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  2500/15247 batches | perplexity    6.849 | loss    1.924 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2992it [00:01, 2719.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  3000/15247 batches | perplexity    7.132 | loss    1.965 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3533it [00:01, 2669.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  3500/15247 batches | perplexity    6.864 | loss    1.926 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3801it [00:01, 2648.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  4000/15247 batches | perplexity    7.046 | loss    1.952 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4339it [00:01, 2659.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  4500/15247 batches | perplexity    7.036 | loss    1.951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4875it [00:01, 2665.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  5000/15247 batches | perplexity    7.010 | loss    1.947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5424it [00:02, 2699.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  5500/15247 batches | perplexity    6.931 | loss    1.936 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5964it [00:02, 2640.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  6000/15247 batches | perplexity    7.000 | loss    1.946 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6516it [00:02, 2620.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  6500/15247 batches | perplexity    6.869 | loss    1.927 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6786it [00:02, 2641.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  7000/15247 batches | perplexity    6.934 | loss    1.936 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7327it [00:02, 2674.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  7500/15247 batches | perplexity    6.962 | loss    1.940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7884it [00:02, 2733.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  8000/15247 batches | perplexity    7.041 | loss    1.952 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8430it [00:03, 2704.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  8500/15247 batches | perplexity    6.900 | loss    1.932 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8974it [00:03, 2704.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  9000/15247 batches | perplexity    6.941 | loss    1.937 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9520it [00:03, 2704.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |  9500/15247 batches | perplexity    7.178 | loss    1.971 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9794it [00:03, 2714.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10000/15247 batches | perplexity    7.172 | loss    1.970 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10331it [00:03, 2610.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 10500/15247 batches | perplexity    7.003 | loss    1.946 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10866it [00:04, 2609.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11000/15247 batches | perplexity    6.907 | loss    1.933 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11397it [00:04, 2611.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11500/15247 batches | perplexity    6.902 | loss    1.932 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11956it [00:04, 2700.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 12000/15247 batches | perplexity    6.855 | loss    1.925 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12508it [00:04, 2681.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 12500/15247 batches | perplexity    6.961 | loss    1.940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12781it [00:04, 2694.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 13000/15247 batches | perplexity    6.978 | loss    1.943 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13331it [00:04, 2723.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 13500/15247 batches | perplexity    6.942 | loss    1.938 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13877it [00:05, 2724.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 14000/15247 batches | perplexity    7.082 | loss    1.958 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14956it [00:05, 2616.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 14500/15247 batches | perplexity    7.103 | loss    1.960 \n",
      "| epoch   2 | 15000/15247 batches | perplexity    6.912 | loss    1.933 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15247it [00:05, 2659.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  5.76s | valid perplexity    7.084 | valid loss    1.958\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "257it [00:00, 2568.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   500/15247 batches | perplexity    6.899 | loss    1.931 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "817it [00:00, 2739.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  1000/15247 batches | perplexity    6.902 | loss    1.932 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1361it [00:00, 2627.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  1500/15247 batches | perplexity    7.139 | loss    1.966 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1889it [00:00, 2626.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  2000/15247 batches | perplexity    6.803 | loss    1.917 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2419it [00:00, 2621.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  2500/15247 batches | perplexity    6.788 | loss    1.915 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2966it [00:01, 2636.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  3000/15247 batches | perplexity    7.003 | loss    1.946 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3514it [00:01, 2686.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  3500/15247 batches | perplexity    7.030 | loss    1.950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3787it [00:01, 2696.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  4000/15247 batches | perplexity    7.103 | loss    1.961 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4330it [00:01, 2705.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  4500/15247 batches | perplexity    7.018 | loss    1.948 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4877it [00:01, 2702.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  5000/15247 batches | perplexity    6.889 | loss    1.930 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5427it [00:02, 2727.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  5500/15247 batches | perplexity    6.909 | loss    1.933 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5978it [00:02, 2742.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  6000/15247 batches | perplexity    7.029 | loss    1.950 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6531it [00:02, 2720.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  6500/15247 batches | perplexity    7.067 | loss    1.955 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6806it [00:02, 2728.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  7000/15247 batches | perplexity    6.963 | loss    1.941 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7361it [00:02, 2754.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  7500/15247 batches | perplexity    7.074 | loss    1.956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7920it [00:02, 2759.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  8000/15247 batches | perplexity    6.960 | loss    1.940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8473it [00:03, 2699.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  8500/15247 batches | perplexity    6.809 | loss    1.918 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9028it [00:03, 2738.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  9000/15247 batches | perplexity    6.998 | loss    1.946 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9306it [00:03, 2749.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |  9500/15247 batches | perplexity    7.069 | loss    1.956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9861it [00:03, 2727.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 10000/15247 batches | perplexity    7.062 | loss    1.955 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10407it [00:03, 2720.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 10500/15247 batches | perplexity    6.924 | loss    1.935 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10951it [00:04, 2686.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 11000/15247 batches | perplexity    7.155 | loss    1.968 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11513it [00:04, 2745.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 11500/15247 batches | perplexity    7.021 | loss    1.949 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11788it [00:04, 2736.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 12000/15247 batches | perplexity    6.948 | loss    1.938 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12335it [00:04, 2721.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 12500/15247 batches | perplexity    6.821 | loss    1.920 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12895it [00:04, 2760.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 13000/15247 batches | perplexity    7.076 | loss    1.957 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13454it [00:04, 2763.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 13500/15247 batches | perplexity    6.744 | loss    1.909 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14016it [00:05, 2787.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 14000/15247 batches | perplexity    7.006 | loss    1.947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14295it [00:05, 2764.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 14500/15247 batches | perplexity    7.110 | loss    1.961 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14849it [00:05, 2760.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 15000/15247 batches | perplexity    7.075 | loss    1.957 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15247it [00:05, 2710.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  5.66s | valid perplexity    7.087 | valid loss    1.958\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261it [00:00, 2601.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   500/15247 batches | perplexity    7.009 | loss    1.947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "803it [00:00, 2677.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1000/15247 batches | perplexity    7.054 | loss    1.954 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1352it [00:00, 2690.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1500/15247 batches | perplexity    6.795 | loss    1.916 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1888it [00:00, 2645.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  2000/15247 batches | perplexity    7.065 | loss    1.955 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2968it [00:01, 2600.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  2500/15247 batches | perplexity    6.752 | loss    1.910 \n",
      "| epoch   4 |  3000/15247 batches | perplexity    6.729 | loss    1.906 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3775it [00:01, 2658.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  3500/15247 batches | perplexity    7.090 | loss    1.959 \n",
      "| epoch   4 |  4000/15247 batches | perplexity    7.056 | loss    1.954 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4888it [00:01, 2756.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  4500/15247 batches | perplexity    6.845 | loss    1.924 \n",
      "| epoch   4 |  5000/15247 batches | perplexity    6.915 | loss    1.934 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6004it [00:02, 2738.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  5500/15247 batches | perplexity    6.898 | loss    1.931 \n",
      "| epoch   4 |  6000/15247 batches | perplexity    6.959 | loss    1.940 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6820it [00:02, 2650.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  6500/15247 batches | perplexity    7.094 | loss    1.959 \n",
      "| epoch   4 |  7000/15247 batches | perplexity    7.034 | loss    1.951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7899it [00:02, 2677.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  7500/15247 batches | perplexity    6.860 | loss    1.926 \n",
      "| epoch   4 |  8000/15247 batches | perplexity    7.074 | loss    1.956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9009it [00:03, 2750.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  8500/15247 batches | perplexity    6.972 | loss    1.942 \n",
      "| epoch   4 |  9000/15247 batches | perplexity    7.017 | loss    1.948 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9829it [00:03, 2700.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  9500/15247 batches | perplexity    6.995 | loss    1.945 \n",
      "| epoch   4 | 10000/15247 batches | perplexity    7.206 | loss    1.975 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10927it [00:04, 2703.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 10500/15247 batches | perplexity    7.001 | loss    1.946 \n",
      "| epoch   4 | 11000/15247 batches | perplexity    7.048 | loss    1.953 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12015it [00:04, 2695.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 11500/15247 batches | perplexity    7.132 | loss    1.965 \n",
      "| epoch   4 | 12000/15247 batches | perplexity    6.945 | loss    1.938 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12829it [00:04, 2686.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 12500/15247 batches | perplexity    6.942 | loss    1.938 \n",
      "| epoch   4 | 13000/15247 batches | perplexity    6.968 | loss    1.941 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13918it [00:05, 2701.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 13500/15247 batches | perplexity    6.848 | loss    1.924 \n",
      "| epoch   4 | 14000/15247 batches | perplexity    7.037 | loss    1.951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15003it [00:05, 2699.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | 14500/15247 batches | perplexity    7.146 | loss    1.967 \n",
      "| epoch   4 | 15000/15247 batches | perplexity    7.089 | loss    1.959 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15247it [00:05, 2679.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  5.72s | valid perplexity    7.081 | valid loss    1.957\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "246it [00:00, 2454.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   500/15247 batches | perplexity    6.994 | loss    1.945 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "789it [00:00, 2652.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  1000/15247 batches | perplexity    6.866 | loss    1.927 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1321it [00:00, 2641.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  1500/15247 batches | perplexity    6.852 | loss    1.925 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1878it [00:00, 2725.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  2000/15247 batches | perplexity    6.941 | loss    1.937 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2424it [00:00, 2716.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  2500/15247 batches | perplexity    7.036 | loss    1.951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2968it [00:01, 2708.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  3000/15247 batches | perplexity    7.012 | loss    1.948 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3510it [00:01, 2693.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  3500/15247 batches | perplexity    7.035 | loss    1.951 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3780it [00:01, 2679.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  4000/15247 batches | perplexity    7.025 | loss    1.949 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4844it [00:01, 2594.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  4500/15247 batches | perplexity    6.965 | loss    1.941 \n",
      "| epoch   5 |  5000/15247 batches | perplexity    6.872 | loss    1.928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5910it [00:02, 2645.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  5500/15247 batches | perplexity    7.065 | loss    1.955 \n",
      "| epoch   5 |  6000/15247 batches | perplexity    7.148 | loss    1.967 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6969it [00:02, 2567.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  6500/15247 batches | perplexity    6.905 | loss    1.932 \n",
      "| epoch   5 |  7000/15247 batches | perplexity    6.893 | loss    1.931 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8046it [00:03, 2671.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  7500/15247 batches | perplexity    6.925 | loss    1.935 \n",
      "| epoch   5 |  8000/15247 batches | perplexity    6.732 | loss    1.907 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8843it [00:03, 2592.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  8500/15247 batches | perplexity    6.997 | loss    1.946 \n",
      "| epoch   5 |  9000/15247 batches | perplexity    7.144 | loss    1.966 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9930it [00:03, 2697.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |  9500/15247 batches | perplexity    6.970 | loss    1.942 \n",
      "| epoch   5 | 10000/15247 batches | perplexity    6.902 | loss    1.932 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11046it [00:04, 2776.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 10500/15247 batches | perplexity    7.122 | loss    1.963 \n",
      "| epoch   5 | 11000/15247 batches | perplexity    7.010 | loss    1.947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11871it [00:04, 2587.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 11500/15247 batches | perplexity    7.045 | loss    1.952 \n",
      "| epoch   5 | 12000/15247 batches | perplexity    7.011 | loss    1.948 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12953it [00:04, 2643.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 12500/15247 batches | perplexity    7.064 | loss    1.955 \n",
      "| epoch   5 | 13000/15247 batches | perplexity    6.975 | loss    1.942 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13785it [00:05, 2694.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 13500/15247 batches | perplexity    7.064 | loss    1.955 \n",
      "| epoch   5 | 14000/15247 batches | perplexity    6.988 | loss    1.944 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14901it [00:05, 2723.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 14500/15247 batches | perplexity    6.790 | loss    1.915 \n",
      "| epoch   5 | 15000/15247 batches | perplexity    7.068 | loss    1.956 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15247it [00:05, 2648.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  5.79s | valid perplexity    7.084 | valid loss    1.958\n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test perplexity    6.974 | test loss    1.942 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    loss_val, perplexity_val = evaluate(validation_dataloader, model, criterion)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} \"\n",
    "        \"| time: {:5.2f}s \"\n",
    "        \"| valid perplexity {:8.3f} \"\n",
    "        \"| valid loss {:8.3f}\".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            perplexity_val,\n",
    "            loss_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "loss_test, perplexity_test =evaluate(test_dataloader, model, criterion)\n",
    "print(\"test perplexity {:8.3f} | test loss {:8.3f} \".format(perplexity_test, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0230d",
   "metadata": {},
   "source": [
    "## Generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6e97642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(model, dataset, window):\n",
    "    generated_word = []\n",
    "    # Set the context to a window-1 length array having just the MARKER character's token_id.\n",
    "    context = dataset.__getitem__(0)[:window-1]\n",
    "    \n",
    "    while True:\n",
    "        logits = model(torch.tensor(context).view(1, -1))\n",
    "        \n",
    "        # Get the probabilities from the logits.\n",
    "        # Hint: softmax!\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        # Get 1 sample from a multinomial having the above probabilities.\n",
    "        token_id = torch.multinomial(probs,1).item()\n",
    "        \n",
    "        # Append the token_id to the generated word.\n",
    "        generated_word.append(token_id)\n",
    "        \n",
    "        # Move the context over 1, drop the first (oldest) token and apped the new one above.\n",
    "        # The size of the resulting context should be the same.\n",
    "        # For exaple, if it was \"[0, 1, 2]\" and you generated 4, it should now be [1, 2, 4].\n",
    "        context = context[1:] + [token_id]\n",
    "        \n",
    "        if token_id == 0:\n",
    "            # If you generate token_id = 0, i.e. '.', break out.\n",
    "            break\n",
    "    # Return and decode the generated word to a string.        \n",
    "    return ''.join(dataset.decode(generated_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "238d0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anzorie.\n",
      "lia.\n",
      "aldin.\n",
      "xalrest.\n",
      "dez.\n",
      "briartai.\n",
      "rielciylend.\n",
      "maderion.\n",
      "caera.\n",
      "dacelian.\n",
      "alalie.\n",
      "shais.\n",
      "mayas.\n",
      "tysi.\n",
      "braxia.\n",
      "tye.\n",
      "karie.\n",
      "gros.\n",
      "auk.\n",
      "kanaran.\n",
      "anyaamius.\n",
      "kelee.\n",
      "har.\n",
      "jami.\n",
      "naeka.\n",
      "reem.\n",
      "kaylen.\n",
      "quyla.\n",
      "namius.\n",
      "bylly.\n",
      "jutram.\n",
      "ahazoriexsunyrin.\n",
      "jen.\n",
      "tirooni.\n",
      "evfiah.\n",
      "rosi.\n",
      "rouitta.\n",
      "ynnlaydon.\n",
      "kenassavoli.\n",
      "wlynn.\n",
      "nalaira.\n",
      "anir.\n",
      "ilyn.\n",
      "marri.\n",
      "alevante.\n",
      "kalyn.\n",
      "desleeshanna.\n",
      "daniullae.\n",
      "rmanaililah.\n",
      "cyle.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "for _ in range(50):\n",
    "    print(generate_word(model, train_dataset, n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
